# AnÃ¡lisis y PredicciÃ³n de Registros de Viajes en Taxi de Nueva York

## DescripciÃ³n del Proyecto

Este proyecto de aprendizaje automÃ¡tico utiliza la metodologÃ­a CRISP-DM para analizar y predecir patrones en los registros de viajes de taxi amarillo de Nueva York. El proyecto procesa datos histÃ³ricos de la ComisiÃ³n de Taxis y Limusinas de NYC (TLC) desde 2009 hasta 2023, aplicando tÃ©cnicas avanzadas de ciencia de datos y machine learning para extraer insights valiosos y crear modelos predictivos.

## Autores

- Luis Salamanca
- Brahian Gonzales

## Fuente de Datos

Los datos provienen de los **Registros de Viajes TLC (Trip Record Data)** de la Ciudad de Nueva York. Los registros de taxis amarillos incluyen campos que capturan:

- Fechas y horas de recogida y destino
- Ubicaciones de recogida y destino (LocationID)
- Distancias de viaje
- Tarifas detalladas (fare, extra, mta_tax, tip_amount, tolls_amount)
- Tipos de tarifa y formas de pago
- NÃºmero de pasajeros reportado por el conductor
- Recargos por congestiÃ³n y aeropuerto

> ðŸ“‹ **Para informaciÃ³n detallada sobre la obtenciÃ³n, estructura y anÃ¡lisis tÃ©cnico de los datos**, consulte la [documentaciÃ³n especÃ­fica de datos raw](data/01_raw/README.md).

### CaracterÃ­sticas del Dataset

- **PerÃ­odo**: 2009-2023 (171 archivos mensuales)
- **Formato**: Archivos Parquet optimizados
- **Volumen**: **1,708,142,581 registros totales** (~1.7 mil millones)
- **Campos**: 44 variables Ãºnicas (18-19 por archivo segÃºn Ã©poca)
- **TamaÃ±o**: **27.8 GB** de datos comprimidos
- **Esquemas**: 3 grupos evolutivos (2009, 2010, 2011-2023)
- **ActualizaciÃ³n**: Datos oficiales publicados por NYC TLC

## AnÃ¡lisis TÃ©cnico del Dataset

### EvoluciÃ³n Temporal del Esquema

El anÃ¡lisis de esquemas revela **3 perÃ­odos distintos** en la estructura de datos:

#### PerÃ­odo 1: 2009-2010 (Esquema Legacy)
- **Variables**: 18 campos
- **CaracterÃ­sticas**: Nombres de columnas descriptivos en inglÃ©s
- **Ejemplo**: `Trip_Pickup_DateTime`, `Passenger_Count`, `Fare_Amt`
- **Volumen**: ~14.9M registros/mes
- **TamaÃ±o**: 448-492 MB/archivo

#### PerÃ­odo 2: 2011-2023 (Esquema EstÃ¡ndar TLC)
- **Variables**: 19 campos estandarizados
- **CaracterÃ­sticas**: Nomenclatura oficial TLC
- **Variables clave**: `VendorID`, `tpep_pickup_datetime`, `PULocationID`
- **Campos monetarios**: `fare_amount`, `tip_amount`, `total_amount`
- **Campos especiales**: `congestion_surcharge` (2014+), `airport_fee` (2018+)

### AnÃ¡lisis de Volumen por AÃ±o

| AÃ±o | Registros (millones) | TamaÃ±o total (GB) | Tendencia |
|-----|---------------------|-------------------|----------|
| 2009-2010 | 339.9M | 10.6 GB | **Pico histÃ³rico** |
| 2011-2015 | 830.3M | 10.1 GB | Volumen mÃ¡ximo |
| 2016-2019 | 432.1M | 5.8 GB | Declive gradual |
| 2020-2021 | 55.6M | 0.8 GB | **Impacto COVID-19** |
| 2022-2023 | 49.0M | 0.7 GB | RecuperaciÃ³n parcial |

**Datos Clave del AnÃ¡lisis:**
- **Variable Ãºnica comÃºn**: Solo `mta_tax` presente en todos los aÃ±os
- **Mejor estrategia**: Consolidar por perÃ­odos (2011-2023 = 1.37B registros)
- **Calidad**: >95% completitud en campos core, anomalÃ­as detectadas

### Variables por PerÃ­odo

**Variables consistentes (2011-2023):**
- `VendorID`: Identificador del proveedor
- `tpep_pickup_datetime` / `tpep_dropoff_datetime`: Timestamps de recogida/destino
- `passenger_count`: NÃºmero de pasajeros
- `trip_distance`: Distancia del viaje
- `PULocationID` / `DOLocationID`: IDs de zonas de recogida/destino
- `fare_amount`, `tip_amount`, `total_amount`: Componentes monetarios

**Variables evolutivas:**
- `congestion_surcharge`: Nula 2011-2013, presente 2014+
- `airport_fee`: Nula hasta 2017, presente 2018+
- Tipos de datos: `passenger_count` cambiÃ³ de `int64` a `double` en 2019

### Recomendaciones de ConsolidaciÃ³n

#### Estrategia Recomendada: NormalizaciÃ³n por PerÃ­odos

1. **Grupo Legacy (2009-2010)**
   - Mapeo manual de nombres de columnas
   - TransformaciÃ³n a esquema estÃ¡ndar TLC
   - Volumen: 29M registros, 940 MB

2. **Grupo Principal (2011-2023)**
   - Esquema homogÃ©neo con 17 variables comunes
   - Manejo de campos evolutivos como opcionales
   - Volumen: 140M registros, 2.3 GB

#### ImplementaciÃ³n TÃ©cnica

```python
# Esquema target unificado (17 variables core)
CORE_SCHEMA = [
    'vendor_id', 'pickup_datetime', 'dropoff_datetime',
    'passenger_count', 'trip_distance', 'pickup_location_id',
    'dropoff_location_id', 'rate_code', 'store_and_fwd_flag',
    'payment_type', 'fare_amount', 'extra', 'mta_tax',
    'tip_amount', 'tolls_amount', 'improvement_surcharge',
    'total_amount'
]

# Campos opcionales por perÃ­odo
OPTIONAL_FIELDS = {
    'congestion_surcharge': '2014+',
    'airport_fee': '2018+'
}
```

### Calidad de Datos Observada

- **Completitud**: >95% en campos core
- **Consistencia**: Tipos de datos estables excepto cambios documentados
- **Integridad**: LocationIDs vÃ¡lidos, timestamps correctos
- **AnomalÃ­as detectadas**: Valores extremos en distancias y tarifas (requieren filtrado)

## MetodologÃ­a CRISP-DM

### 1. ComprensiÃ³n del Negocio
- **Objetivo**: Desarrollar modelos predictivos para optimizar operaciones de taxis
- **Casos de uso**: PredicciÃ³n de tarifas, anÃ¡lisis de patrones temporales, optimizaciÃ³n de rutas

### 2. ComprensiÃ³n de los Datos
- **AnÃ¡lisis Exploratorio (EDA)**: Distribuciones, correlaciones, patrones temporales
- **Calidad de datos**: IdentificaciÃ³n de valores nulos, outliers, inconsistencias
- **SegmentaciÃ³n**: AnÃ¡lisis por zonas, horarios, tipos de viaje

### 3. PreparaciÃ³n de los Datos
- **Limpieza**: Tratamiento de valores faltantes y outliers
- **TransformaciÃ³n**: CodificaciÃ³n de variables categÃ³ricas
- **IngenierÃ­a de caracterÃ­sticas**: CreaciÃ³n de variables derivadas

### 4. Modelado
- **Algoritmos implementados**:
  - RegresiÃ³n Lineal
  - Random Forest
  - XGBoost
  - LightGBM

### 5. EvaluaciÃ³n
- **MÃ©tricas**: RMSE, MAE, RÂ²
- **ValidaciÃ³n**: DivisiÃ³n train/validation/test (64%/16%/20%)
- **ComparaciÃ³n de modelos**: AnÃ¡lisis de rendimiento

### 6. Despliegue
- **Contenedores Docker**: OrquestaciÃ³n completa del pipeline
- **VisualizaciÃ³n**: Kedro Viz para monitoreo del pipeline
- **Notebooks**: AnÃ¡lisis interactivo con Jupyter Lab

## Arquitectura del Proyecto

### Estructura de Directorios

```
TaxiTripRecords/
â”œâ”€â”€ data/                           # GestiÃ³n de datos por capas
â”‚   â”œâ”€â”€ 01_raw/                     # Datos originales (Parquet files)
â”‚   â”œâ”€â”€ 02_intermediate/            # Datos procesados
â”‚   â”œâ”€â”€ 03_primary/                 # Datos primarios limpios
â”‚   â”œâ”€â”€ 04_feature/                 # CaracterÃ­sticas engineered
â”‚   â”œâ”€â”€ 05_model_input/             # Datos para entrenamiento
â”‚   â”œâ”€â”€ 06_models/                  # Modelos entrenados
â”‚   â”œâ”€â”€ 07_model_output/            # Predicciones
â”‚   â””â”€â”€ 08_reporting/               # Reportes y mÃ©tricas
â”œâ”€â”€ src/taxi_ml/                    # CÃ³digo fuente
â”‚   â”œâ”€â”€ pipelines/
â”‚   â”‚   â”œâ”€â”€ data_processing/        # Pipeline de procesamiento
â”‚   â”‚   â””â”€â”€ data_science/           # Pipeline de ML
â”‚   â””â”€â”€ pipeline_registry.py       # Registro de pipelines
â”œâ”€â”€ conf/                           # Configuraciones
â”‚   â”œâ”€â”€ base/
â”‚   â”‚   â”œâ”€â”€ catalog.yml             # CatÃ¡logo de datasets
â”‚   â”‚   â””â”€â”€ parameters.yml          # HiperparÃ¡metros
â”‚   â””â”€â”€ local/                      # Configuraciones locales
â”œâ”€â”€ notebooks/                      # AnÃ¡lisis exploratorio
â”œâ”€â”€ tests/                          # Tests automatizados
â””â”€â”€ docker-compose.yml             # OrquestaciÃ³n de servicios
```

### Pipelines Kedro

#### Pipeline de Procesamiento de Datos
```
Datos Raw â†’ Limpieza â†’ TransformaciÃ³n â†’ DivisiÃ³n â†’ Datos de Entrada
```

**Nodos principales**:
- `preprocess_taxi_data()`: Limpieza y transformaciÃ³n inicial
- `split_data()`: DivisiÃ³n en conjuntos train/val/test

#### Pipeline de Ciencia de Datos
```
IngenierÃ­a de CaracterÃ­sticas â†’ Entrenamiento â†’ EvaluaciÃ³n â†’ ComparaciÃ³n
```

**Nodos principales**:
- `prepare_features()`: CodificaciÃ³n y escalado de caracterÃ­sticas
- `train_*_model()`: Entrenamiento de 4 algoritmos ML
- `evaluate_model()`: CÃ¡lculo de mÃ©tricas de rendimiento
- `compare_models()`: Reporte comparativo de modelos

### Flujo de Datos

```mermaid
graph LR
    A[Datos Raw] --> B[Procesamiento]
    B --> C[DivisiÃ³n Train/Val/Test]
    C --> D[IngenierÃ­a de CaracterÃ­sticas]
    D --> E[Entrenamiento ML]
    E --> F[EvaluaciÃ³n]
    F --> G[ComparaciÃ³n de Modelos]
```

## Comandos de EjecuciÃ³n

### Usando Docker Compose (Recomendado)

#### EjecuciÃ³n Completa del Pipeline
```bash
# Pipeline completo de ML
docker-compose up airlines-ml

# Solo procesamiento de datos
docker-compose up data-processing

# Solo entrenamiento ML
docker-compose up ml-training
```

#### Servicios de Desarrollo
```bash
# VisualizaciÃ³n del pipeline (http://localhost:4141)
docker-compose up kedro-viz

# Jupyter Lab para anÃ¡lisis (http://localhost:8888)
docker-compose up jupyter

# Contenedor de desarrollo interactivo
docker-compose run --rm dev
```

#### GestiÃ³n de Servicios
```bash
# Construir todos los servicios
docker-compose build

# Detener todos los servicios
docker-compose down

# Ejecutar tests
docker-compose up test
```

### EjecuciÃ³n Local

#### Pipeline Operations
```bash
# Pipeline completo
kedro run

# Solo procesamiento de datos
kedro run --pipeline data_processing

# Solo machine learning
kedro run --pipeline data_science

# Reanudar desde nodo especÃ­fico
kedro run --from-nodes "nombre_del_nodo"
```

#### Herramientas de Desarrollo
```bash
# Instalar dependencias
pip install -r requirements.txt
pip install -e .

# Visualizar pipeline
kedro viz

# Jupyter para exploraciÃ³n
kedro jupyter notebook

# Ejecutar tests
pytest

# Verificar calidad de cÃ³digo
kedro lint
```

#### GestiÃ³n de Datos
```bash
# Listar datasets disponibles
kedro catalog list

# Describir dataset especÃ­fico
kedro catalog describe taxi_raw
```

## TecnologÃ­as y Herramientas

### Frameworks y LibrerÃ­as
- **Kedro**: OrquestaciÃ³n de pipelines de ML
- **Pandas**: ManipulaciÃ³n y anÃ¡lisis de datos
- **Scikit-learn**: Algoritmos de machine learning
- **XGBoost**: Gradient boosting optimizado
- **LightGBM**: Gradient boosting eficiente

### Infraestructura
- **Docker & Docker Compose**: ContenedorizaciÃ³n y orquestaciÃ³n
- **Jupyter Lab**: Desarrollo interactivo
- **Kedro Viz**: VisualizaciÃ³n de pipelines
- **pytest**: Framework de testing

### Almacenamiento de Datos
- **Parquet**: Formato columnar optimizado
- **Pickle**: SerializaciÃ³n de modelos
- **CSV**: Reportes y mÃ©tricas

## Estructura de Archivos de ConfiguraciÃ³n

### CatÃ¡logo de Datos (`conf/base/catalog.yml`)
Define todos los datasets desde datos raw hasta mÃ©tricas finales:
- Datasets de entrada: formato Parquet
- Modelos y encoders: formato Pickle
- Reportes: formato CSV

### ParÃ¡metros (`conf/base/parameters.yml`)
Contiene hiperparÃ¡metros para todos los algoritmos ML:
- Random Forest: n_estimators, max_depth, min_samples_split
- XGBoost: learning_rate, subsample, colsample_bytree
- LightGBM: num_leaves, feature_fraction

## MÃ©tricas y EvaluaciÃ³n

### MÃ©tricas Implementadas
- **RMSE** (Root Mean Square Error)
- **MAE** (Mean Absolute Error)
- **RÂ²** (Coeficiente de DeterminaciÃ³n)

### Proceso de ValidaciÃ³n
1. DivisiÃ³n estratificada de datos (64%/16%/20%)
2. Entrenamiento en conjunto de entrenamiento
3. ValidaciÃ³n en conjunto de validaciÃ³n
4. EvaluaciÃ³n final en conjunto de prueba
5. ComparaciÃ³n cruzada de todos los modelos

## Casos de Uso

### AnÃ¡lisis Descriptivo
- Patrones temporales de demanda de taxis
- AnÃ¡lisis geogrÃ¡fico por zonas de NYC
- DistribuciÃ³n de tarifas y propinas
- Comportamiento de usuarios por tipo de pago

### Modelos Predictivos
- **PredicciÃ³n de tarifas**: EstimaciÃ³n de costos de viaje
- **AnÃ¡lisis de demanda**: Patrones de uso por zona/hora
- **OptimizaciÃ³n de rutas**: IdentificaciÃ³n de rutas eficientes
- **DetecciÃ³n de anomalÃ­as**: Viajes inusuales o fraudulentos

## Requisitos del Sistema

### Dependencias Python
- Python >= 3.9
- NumPy < 2.0 (compatibilidad con librerÃ­as ML)
- Pandas para manipulaciÃ³n de datos
- Scikit-learn para algoritmos ML base
- Kedro ~= 1.0.0 para orquestaciÃ³n

### Recursos Computacionales
- RAM: MÃ­nimo 8GB (recomendado 16GB+)
- Almacenamiento: 50GB+ para datasets completos
- CPU: Multi-core recomendado para entrenamiento

## Testing

### Estructura de Tests
```bash
tests/
â”œâ”€â”€ pipelines/
â”‚   â”œâ”€â”€ data_processing/
â”‚   â””â”€â”€ data_science/
â””â”€â”€ test_run.py
```

### Ejecutar Tests
```bash
# Tests locales
pytest

# Tests en contenedor
docker-compose up test
```

## PrÃ³ximos Pasos

### Mejoras Planificadas
- ImplementaciÃ³n de deep learning (Neural Networks)
- AnÃ¡lisis de series temporales para predicciÃ³n de demanda
- IntegraciÃ³n con APIs de mapas para anÃ¡lisis geoespacial
- Dashboard interactivo para visualizaciÃ³n de resultados

### Optimizaciones
- Procesamiento distribuido con Dask
- Hyperparameter tuning automatizado
- MLOps con tracking de experimentos (MLflow)
- Despliegue de modelos como APIs

## Contribuciones

Este proyecto sigue las mejores prÃ¡cticas de desarrollo de software:
- CÃ³digo versionado con Git
- Tests automatizados
- DocumentaciÃ³n tÃ©cnica detallada
- ContainerizaciÃ³n para reproducibilidad

## Licencia

Proyecto acadÃ©mico con fines educativos y de investigaciÃ³n en ciencia de datos aplicada al transporte urbano.

---

*Desarrollado con Kedro Framework y metodologÃ­a CRISP-DM para anÃ¡lisis profesional de datos de transporte urbano.*