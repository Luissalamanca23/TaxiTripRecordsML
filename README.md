# An√°lisis y Predicci√≥n de Registros de Viajes en Taxi de Nueva York

## Descripci√≥n del Proyecto

Este proyecto de aprendizaje autom√°tico utiliza la metodolog√≠a CRISP-DM para analizar y predecir patrones en los registros de viajes de taxi amarillo de Nueva York. El proyecto procesa datos hist√≥ricos de la Comisi√≥n de Taxis y Limusinas de NYC (TLC) desde 2009 hasta 2023, aplicando t√©cnicas avanzadas de ciencia de datos y machine learning para extraer insights valiosos y crear modelos predictivos.

## Autores

- Luis Salamanca
- Brahian Gonzales

## Fuente de Datos

Los datos provienen de los **Registros de Viajes TLC (Trip Record Data)** de la Ciudad de Nueva York. Los registros de taxis amarillos incluyen campos que capturan:

- Fechas y horas de recogida y destino
- Ubicaciones de recogida y destino (LocationID)
- Distancias de viaje
- Tarifas detalladas (fare, extra, mta_tax, tip_amount, tolls_amount)
- Tipos de tarifa y formas de pago
- N√∫mero de pasajeros reportado por el conductor
- Recargos por congesti√≥n y aeropuerto

> üìã **Para informaci√≥n detallada sobre la obtenci√≥n, estructura y an√°lisis t√©cnico de los datos**, consulte la [documentaci√≥n espec√≠fica de datos raw](data/01_raw/README.md).

### Caracter√≠sticas del Dataset

- **Per√≠odo**: 2009-2023 (171 archivos mensuales)
- **Formato**: Archivos Parquet optimizados
- **Volumen**: **1,708,142,581 registros totales** (~1.7 mil millones)
- **Campos**: 44 variables √∫nicas (18-19 por archivo seg√∫n √©poca)
- **Tama√±o**: **27.8 GB** de datos comprimidos
- **Esquemas**: 3 grupos evolutivos (2009, 2010, 2011-2023)
- **Actualizaci√≥n**: Datos oficiales publicados por NYC TLC

## An√°lisis T√©cnico del Dataset

### Evoluci√≥n Temporal del Esquema

El an√°lisis de esquemas revela **3 per√≠odos distintos** en la estructura de datos:

#### Per√≠odo 1: 2009-2010 (Esquema Legacy)
- **Variables**: 18 campos
- **Caracter√≠sticas**: Nombres de columnas descriptivos en ingl√©s
- **Ejemplo**: `Trip_Pickup_DateTime`, `Passenger_Count`, `Fare_Amt`
- **Volumen**: ~14.9M registros/mes
- **Tama√±o**: 448-492 MB/archivo

#### Per√≠odo 2: 2011-2023 (Esquema Est√°ndar TLC)
- **Variables**: 19 campos estandarizados
- **Caracter√≠sticas**: Nomenclatura oficial TLC
- **Variables clave**: `VendorID`, `tpep_pickup_datetime`, `PULocationID`
- **Campos monetarios**: `fare_amount`, `tip_amount`, `total_amount`
- **Campos especiales**: `congestion_surcharge` (2014+), `airport_fee` (2018+)

### An√°lisis de Volumen por A√±o

| A√±o | Registros (millones) | Tama√±o total (GB) | Tendencia |
|-----|---------------------|-------------------|----------|
| 2009-2010 | 339.9M | 10.6 GB | **Pico hist√≥rico** |
| 2011-2015 | 830.3M | 10.1 GB | Volumen m√°ximo |
| 2016-2019 | 432.1M | 5.8 GB | Declive gradual |
| 2020-2021 | 55.6M | 0.8 GB | **Impacto COVID-19** |
| 2022-2023 | 49.0M | 0.7 GB | Recuperaci√≥n parcial |

**Datos Clave del An√°lisis:**
- **Variable √∫nica com√∫n**: Solo `mta_tax` presente en todos los a√±os
- **Mejor estrategia**: Consolidar por per√≠odos (2011-2023 = 1.37B registros)
- **Calidad**: >95% completitud en campos core, anomal√≠as detectadas

### Variables por Per√≠odo

**Variables consistentes (2011-2023):**
- `VendorID`: Identificador del proveedor
- `tpep_pickup_datetime` / `tpep_dropoff_datetime`: Timestamps de recogida/destino
- `passenger_count`: N√∫mero de pasajeros
- `trip_distance`: Distancia del viaje
- `PULocationID` / `DOLocationID`: IDs de zonas de recogida/destino
- `fare_amount`, `tip_amount`, `total_amount`: Componentes monetarios

**Variables evolutivas:**
- `congestion_surcharge`: Nula 2011-2013, presente 2014+
- `airport_fee`: Nula hasta 2017, presente 2018+
- Tipos de datos: `passenger_count` cambi√≥ de `int64` a `double` en 2019

### Recomendaciones de Consolidaci√≥n

#### Estrategia Recomendada: Normalizaci√≥n por Per√≠odos

1. **Grupo Legacy (2009-2010)**
   - Mapeo manual de nombres de columnas
   - Transformaci√≥n a esquema est√°ndar TLC
   - Volumen: 29M registros, 940 MB

2. **Grupo Principal (2011-2023)**
   - Esquema homog√©neo con 17 variables comunes
   - Manejo de campos evolutivos como opcionales
   - Volumen: 140M registros, 2.3 GB

#### Implementaci√≥n T√©cnica

```python
# Esquema target unificado (17 variables core)
CORE_SCHEMA = [
    'vendor_id', 'pickup_datetime', 'dropoff_datetime',
    'passenger_count', 'trip_distance', 'pickup_location_id',
    'dropoff_location_id', 'rate_code', 'store_and_fwd_flag',
    'payment_type', 'fare_amount', 'extra', 'mta_tax',
    'tip_amount', 'tolls_amount', 'improvement_surcharge',
    'total_amount'
]

# Campos opcionales por per√≠odo
OPTIONAL_FIELDS = {
    'congestion_surcharge': '2014+',
    'airport_fee': '2018+'
}
```

### Calidad de Datos Observada

- **Completitud**: >95% en campos core
- **Consistencia**: Tipos de datos estables excepto cambios documentados
- **Integridad**: LocationIDs v√°lidos, timestamps correctos
- **Anomal√≠as detectadas**: Valores extremos en distancias y tarifas (requieren filtrado)

## Metodolog√≠a CRISP-DM

### 1. Comprensi√≥n del Negocio
- **Objetivo**: Desarrollar modelos predictivos para optimizar operaciones de taxis
- **Casos de uso**: Predicci√≥n de tarifas, an√°lisis de patrones temporales, optimizaci√≥n de rutas

### 2. Comprensi√≥n de los Datos
- **An√°lisis Exploratorio (EDA)**: Distribuciones, correlaciones, patrones temporales
- **Calidad de datos**: Identificaci√≥n de valores nulos, outliers, inconsistencias
- **Segmentaci√≥n**: An√°lisis por zonas, horarios, tipos de viaje

### 3. Preparaci√≥n de los Datos
- **Limpieza**: Tratamiento de valores faltantes y outliers
- **Transformaci√≥n**: Codificaci√≥n de variables categ√≥ricas
- **Ingenier√≠a de caracter√≠sticas**: Creaci√≥n de variables derivadas

### 4. Modelado
- **Algoritmos implementados**:
  - Regresi√≥n Lineal
  - Random Forest
  - XGBoost
  - LightGBM

### 5. Evaluaci√≥n
- **M√©tricas**: RMSE, MAE, R¬≤
- **Validaci√≥n**: Divisi√≥n train/validation/test (64%/16%/20%)
- **Comparaci√≥n de modelos**: An√°lisis de rendimiento

### 6. Despliegue
- **Contenedores Docker**: Orquestaci√≥n completa del pipeline
- **Visualizaci√≥n**: Kedro Viz para monitoreo del pipeline
- **Notebooks**: An√°lisis interactivo con Jupyter Lab

## Arquitectura del Proyecto

### Estructura de Directorios

```
TaxiTripRecords/
‚îú‚îÄ‚îÄ data/                           # Gesti√≥n de datos por capas
‚îÇ   ‚îú‚îÄ‚îÄ 01_raw/                     # Datos originales (Parquet files)
‚îÇ   ‚îú‚îÄ‚îÄ 02_intermediate/            # Datos procesados
‚îÇ   ‚îú‚îÄ‚îÄ 03_primary/                 # Datos primarios limpios
‚îÇ   ‚îú‚îÄ‚îÄ 04_feature/                 # Caracter√≠sticas engineered
‚îÇ   ‚îú‚îÄ‚îÄ 05_model_input/             # Datos para entrenamiento
‚îÇ   ‚îú‚îÄ‚îÄ 06_models/                  # Modelos entrenados
‚îÇ   ‚îú‚îÄ‚îÄ 07_model_output/            # Predicciones
‚îÇ   ‚îî‚îÄ‚îÄ 08_reporting/               # Reportes y m√©tricas
‚îú‚îÄ‚îÄ src/taxi_ml/                    # C√≥digo fuente
‚îÇ   ‚îú‚îÄ‚îÄ pipelines/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data_processing/        # Pipeline de procesamiento
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ data_science/           # Pipeline de ML
‚îÇ   ‚îî‚îÄ‚îÄ pipeline_registry.py       # Registro de pipelines
‚îú‚îÄ‚îÄ conf/                           # Configuraciones
‚îÇ   ‚îú‚îÄ‚îÄ base/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ catalog.yml             # Cat√°logo de datasets
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ parameters.yml          # Hiperpar√°metros
‚îÇ   ‚îî‚îÄ‚îÄ local/                      # Configuraciones locales
‚îú‚îÄ‚îÄ notebooks/                      # An√°lisis exploratorio
‚îú‚îÄ‚îÄ tests/                          # Tests automatizados
‚îî‚îÄ‚îÄ docker-compose.yml             # Orquestaci√≥n de servicios
```

### Pipelines Kedro

#### Pipeline de Procesamiento de Datos
```
Datos Raw ‚Üí Limpieza ‚Üí Transformaci√≥n ‚Üí Divisi√≥n ‚Üí Datos de Entrada
```

**Nodos principales**:
- `preprocess_taxi_data()`: Limpieza y transformaci√≥n inicial
- `split_data()`: Divisi√≥n en conjuntos train/val/test

#### Pipeline de Ciencia de Datos
```
Ingenier√≠a de Caracter√≠sticas ‚Üí Entrenamiento ‚Üí Evaluaci√≥n ‚Üí Comparaci√≥n
```

**Nodos principales**:
- `prepare_features()`: Codificaci√≥n y escalado de caracter√≠sticas
- `train_*_model()`: Entrenamiento de 4 algoritmos ML
- `evaluate_model()`: C√°lculo de m√©tricas de rendimiento
- `compare_models()`: Reporte comparativo de modelos

### Flujo de Datos

```mermaid
graph LR
    A[Datos Raw] --> B[Procesamiento]
    B --> C[Divisi√≥n Train/Val/Test]
    C --> D[Ingenier√≠a de Caracter√≠sticas]
    D --> E[Entrenamiento ML]
    E --> F[Evaluaci√≥n]
    F --> G[Comparaci√≥n de Modelos]
```

## Comandos de Ejecuci√≥n

### Usando Docker Compose (Recomendado)

#### Ejecuci√≥n Completa del Pipeline
```bash
# Pipeline completo de ML
docker-compose up airlines-ml

# Solo procesamiento de datos
docker-compose up data-processing

# Solo entrenamiento ML
docker-compose up ml-training
```

#### Servicios de Desarrollo
```bash
# Visualizaci√≥n del pipeline (http://localhost:4141)
docker-compose up kedro-viz

# Jupyter Lab para an√°lisis (http://localhost:8888)
docker-compose up jupyter

# Contenedor de desarrollo interactivo
docker-compose run --rm dev
```

#### Gesti√≥n de Servicios
```bash
# Construir todos los servicios
docker-compose build

# Detener todos los servicios
docker-compose down

# Ejecutar tests
docker-compose up test
```

### Ejecuci√≥n Local

#### Pipeline Operations
```bash
# Pipeline completo
kedro run

# Solo procesamiento de datos
kedro run --pipeline data_processing

# Solo machine learning
kedro run --pipeline data_science

# Reanudar desde nodo espec√≠fico
kedro run --from-nodes "nombre_del_nodo"
```

#### Herramientas de Desarrollo
```bash
# Instalar dependencias
pip install -r requirements.txt
pip install -e .

# Visualizar pipeline
kedro viz

# Jupyter para exploraci√≥n
kedro jupyter notebook

# Ejecutar tests
pytest

# Verificar calidad de c√≥digo
kedro lint
```

#### Gesti√≥n de Datos
```bash
# Listar datasets disponibles
kedro catalog list

# Describir dataset espec√≠fico
kedro catalog describe taxi_raw
```

## Tecnolog√≠as y Herramientas

### Frameworks y Librer√≠as
- **Kedro**: Orquestaci√≥n de pipelines de ML
- **Pandas**: Manipulaci√≥n y an√°lisis de datos
- **Scikit-learn**: Algoritmos de machine learning
- **XGBoost**: Gradient boosting optimizado
- **LightGBM**: Gradient boosting eficiente

### Infraestructura
- **Docker & Docker Compose**: Contenedorizaci√≥n y orquestaci√≥n
- **Jupyter Lab**: Desarrollo interactivo
- **Kedro Viz**: Visualizaci√≥n de pipelines
- **pytest**: Framework de testing

### Almacenamiento de Datos
- **Parquet**: Formato columnar optimizado
- **Pickle**: Serializaci√≥n de modelos
- **CSV**: Reportes y m√©tricas

## Estructura de Archivos de Configuraci√≥n

### Cat√°logo de Datos (`conf/base/catalog.yml`)
Define todos los datasets desde datos raw hasta m√©tricas finales:
- Datasets de entrada: formato Parquet
- Modelos y encoders: formato Pickle
- Reportes: formato CSV

### Par√°metros (`conf/base/parameters.yml`)
Contiene hiperpar√°metros para todos los algoritmos ML:
- Random Forest: n_estimators, max_depth, min_samples_split
- XGBoost: learning_rate, subsample, colsample_bytree
- LightGBM: num_leaves, feature_fraction

## M√©tricas y Evaluaci√≥n

### M√©tricas Implementadas
- **RMSE** (Root Mean Square Error)
- **MAE** (Mean Absolute Error)
- **R¬≤** (Coeficiente de Determinaci√≥n)

### Proceso de Validaci√≥n
1. Divisi√≥n estratificada de datos (64%/16%/20%)
2. Entrenamiento en conjunto de entrenamiento
3. Validaci√≥n en conjunto de validaci√≥n
4. Evaluaci√≥n final en conjunto de prueba
5. Comparaci√≥n cruzada de todos los modelos

## Casos de Uso

### An√°lisis Descriptivo
- Patrones temporales de demanda de taxis
- An√°lisis geogr√°fico por zonas de NYC
- Distribuci√≥n de tarifas y propinas
- Comportamiento de usuarios por tipo de pago

### Modelos Predictivos
- **Predicci√≥n de tarifas**: Estimaci√≥n de costos de viaje
- **An√°lisis de demanda**: Patrones de uso por zona/hora
- **Optimizaci√≥n de rutas**: Identificaci√≥n de rutas eficientes
- **Detecci√≥n de anomal√≠as**: Viajes inusuales o fraudulentos

## Requisitos del Sistema

### Dependencias Python
- Python >= 3.9
- NumPy < 2.0 (compatibilidad con librer√≠as ML)
- Pandas para manipulaci√≥n de datos
- Scikit-learn para algoritmos ML base
- Kedro ~= 1.0.0 para orquestaci√≥n

### Recursos Computacionales
- RAM: M√≠nimo 8GB (recomendado 16GB+)
- Almacenamiento: 50GB+ para datasets completos
- CPU: Multi-core recomendado para entrenamiento

## Testing

### Estructura de Tests
```bash
tests/
‚îú‚îÄ‚îÄ pipelines/
‚îÇ   ‚îú‚îÄ‚îÄ data_processing/
‚îÇ   ‚îî‚îÄ‚îÄ data_science/
‚îî‚îÄ‚îÄ test_run.py
```

### Ejecutar Tests
```bash
# Tests locales
pytest

# Tests en contenedor
docker-compose up test
```

## Pr√≥ximos Pasos

### Mejoras Planificadas
- Implementaci√≥n de deep learning (Neural Networks)
- An√°lisis de series temporales para predicci√≥n de demanda
- Integraci√≥n con APIs de mapas para an√°lisis geoespacial
- Dashboard interactivo para visualizaci√≥n de resultados

### Optimizaciones
- Procesamiento distribuido con Dask
- Hyperparameter tuning automatizado
- MLOps con tracking de experimentos (MLflow)
- Despliegue de modelos como APIs

## Contribuciones

Este proyecto sigue las mejores pr√°cticas de desarrollo de software:
- C√≥digo versionado con Git
- Tests automatizados
- Documentaci√≥n t√©cnica detallada
- Containerizaci√≥n para reproducibilidad

## Licencia

Proyecto acad√©mico con fines educativos y de investigaci√≥n en ciencia de datos aplicada al transporte urbano.

---

*Desarrollado con Kedro Framework y metodolog√≠a CRISP-DM para an√°lisis profesional de datos de transporte urbano.*